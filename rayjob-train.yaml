apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: raytrain-fashion-mnist
  namespace: default
spec:
  # The entrypoint command to run the training job
  entrypoint: python /app/train.py

  # Runtime environment for the job
  runtimeEnvYAML: |
    pip:
      - torch==2.1.0
      - torchvision==0.16.0
    env_vars:
      RAY_LOG_TO_STDERR: "1"

  # Shutdown behavior after job completes
  shutdownAfterJobFinishes: true
  ttlSecondsAfterFinished: 600

  # Ray cluster configuration
  rayClusterSpec:
    rayVersion: '2.9.0'

    # Head node configuration
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
        block: 'true'
      template:
        spec:
          containers:
          - name: ray-head
            image: rayproject/ray:2.9.0-py310
            imagePullPolicy: IfNotPresent
            ports:
            - containerPort: 6379
              name: gcs
            - containerPort: 8265
              name: dashboard
            - containerPort: 10001
              name: client
            resources:
              limits:
                cpu: "2"
                memory: "4Gi"
              requests:
                cpu: "1"
                memory: "2Gi"
            volumeMounts:
            - name: code
              mountPath: /app
          volumes:
          - name: code
            configMap:
              name: raytrain-code

    # Worker node configuration
    workerGroupSpecs:
    - replicas: 2
      minReplicas: 1
      maxReplicas: 4
      groupName: worker-group
      rayStartParams:
        block: 'true'
      template:
        spec:
          containers:
          - name: ray-worker
            image: rayproject/ray:2.9.0-py310
            imagePullPolicy: IfNotPresent
            resources:
              limits:
                cpu: "2"
                memory: "4Gi"
              requests:
                cpu: "1"
                memory: "2Gi"
            volumeMounts:
            - name: code
              mountPath: /app
          volumes:
          - name: code
            configMap:
              name: raytrain-code

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: raytrain-code
  namespace: default
data:
  train.py: |
    """
    Ray Train example for distributed PyTorch training on Kubernetes.
    This example trains a simple neural network on the Fashion MNIST dataset.
    """

    import os
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import DataLoader
    from torchvision import datasets, transforms

    import ray
    from ray import train
    from ray.train import ScalingConfig, RunConfig, CheckpointConfig
    from ray.train.torch import TorchTrainer


    # Define a simple neural network
    class Net(nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.conv1 = nn.Conv2d(1, 32, 3, 1)
            self.conv2 = nn.Conv2d(32, 64, 3, 1)
            self.dropout1 = nn.Dropout2d(0.25)
            self.dropout2 = nn.Dropout2d(0.5)
            self.fc1 = nn.Linear(9216, 128)
            self.fc2 = nn.Linear(128, 10)

        def forward(self, x):
            x = self.conv1(x)
            x = F.relu(x)
            x = self.conv2(x)
            x = F.relu(x)
            x = F.max_pool2d(x, 2)
            x = self.dropout1(x)
            x = torch.flatten(x, 1)
            x = self.fc1(x)
            x = F.relu(x)
            x = self.dropout2(x)
            x = self.fc2(x)
            output = F.log_softmax(x, dim=1)
            return output


    def train_func(config):
        """Training function to be distributed across workers."""

        # Get training configuration
        batch_size = config.get("batch_size", 64)
        lr = config.get("lr", 0.001)
        epochs = config.get("epochs", 5)

        # Prepare datasets
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])

        train_dataset = datasets.FashionMNIST(
            root="/tmp/data",
            train=True,
            download=True,
            transform=transform
        )

        test_dataset = datasets.FashionMNIST(
            root="/tmp/data",
            train=False,
            download=True,
            transform=transform
        )

        # Prepare data loaders
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True
        )

        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False
        )

        # Prepare model, optimizer, and data loaders for distributed training
        model = Net()
        model = train.torch.prepare_model(model)

        optimizer = torch.optim.Adam(model.parameters(), lr=lr)

        train_loader = train.torch.prepare_data_loader(train_loader)
        test_loader = train.torch.prepare_data_loader(test_loader)

        # Training loop
        for epoch in range(epochs):
            model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0

            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()
                output = model(data)
                loss = F.nll_loss(output, target)
                loss.backward()
                optimizer.step()

                train_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                train_correct += pred.eq(target.view_as(pred)).sum().item()
                train_total += target.size(0)

                if batch_idx % 100 == 0:
                    print(f"Epoch {epoch+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}")

            # Validation
            model.eval()
            test_loss = 0.0
            test_correct = 0
            test_total = 0

            with torch.no_grad():
                for data, target in test_loader:
                    output = model(data)
                    test_loss += F.nll_loss(output, target, reduction='sum').item()
                    pred = output.argmax(dim=1, keepdim=True)
                    test_correct += pred.eq(target.view_as(pred)).sum().item()
                    test_total += target.size(0)

            train_loss = train_loss / len(train_loader)
            train_acc = train_correct / train_total
            test_loss = test_loss / test_total
            test_acc = test_correct / test_total

            print(f"\nEpoch {epoch+1}/{epochs}:")
            print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
            print(f"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\n")

            # Report metrics and checkpoint
            checkpoint = None
            if train.get_context().get_world_rank() == 0:
                checkpoint = train.torch.TorchCheckpoint.from_model(model)

            train.report(
                metrics={
                    "epoch": epoch + 1,
                    "train_loss": train_loss,
                    "train_acc": train_acc,
                    "test_loss": test_loss,
                    "test_acc": test_acc
                },
                checkpoint=checkpoint
            )

        print("Training completed!")


    def main():
        """Main function to setup and run Ray Train."""

        # Initialize Ray
        ray.init(
            runtime_env={
                "pip": [
                    "torch==2.1.0",
                    "torchvision==0.16.0"
                ]
            }
        )

        # Configure the trainer
        scaling_config = ScalingConfig(
            num_workers=2,  # Number of distributed training workers
            use_gpu=False,  # Set to True if GPUs are available
            resources_per_worker={
                "CPU": 2,
                "GPU": 0  # Change to 1 if using GPUs
            }
        )

        run_config = RunConfig(
            name="fashion_mnist_train",
            storage_path="/tmp/ray_results",
            checkpoint_config=CheckpointConfig(
                num_to_keep=2,
                checkpoint_score_attribute="test_acc",
                checkpoint_score_order="max"
            )
        )

        # Create the trainer
        trainer = TorchTrainer(
            train_loop_per_worker=train_func,
            train_loop_config={
                "batch_size": 64,
                "lr": 0.001,
                "epochs": 5
            },
            scaling_config=scaling_config,
            run_config=run_config
        )

        # Run training
        result = trainer.fit()

        print("\n" + "="*50)
        print("Training Results:")
        print("="*50)
        print(f"Best checkpoint: {result.checkpoint}")
        print(f"Best metrics: {result.metrics}")
        print("="*50)


    if __name__ == "__main__":
        main()
